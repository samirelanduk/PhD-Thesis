%%%% MACRO DEFINITION %%%%
% if any ...


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%													BEGIN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{atomium} % Write in your own chapter title
\lhead{Appendix. \emph{A}} % Write in your own chapter title to set the page header

%TC:macro \note [ignore]

% write your code here
Much of this project relies very heavily on reading in structure data from files deposited to the Protein Data Bank.

Traditionally these files have been stored and distributed in the form of .pdb files. These are text files that consist of a list of records, with each record being limited to 80 characters. Information is stored in these records at fixed offsets from the start of the line. This file format has a number of limitations - most notably the fact that they cannot store more than 100,000 atoms because only five characters are allocated to atom IDs, and so they cannot go past 99,999.

Consequently, the Protein Data Bank also provides files in the newer .cif file format - and indeed, structures with more than 100,000 atoms are \emph{only} available in this file format.

There are a number of parsers available that can handle these file formats, and which were considered for use in this project - both in Python and otherwise. However, for reasons that will be outlined shortly, it was decided to create a novel Python parser. This parser is called atomium.

\section{Rationale}

The tool used to read structure files is of such critical importance to this project, that it was important to ensure that the correct tool with the correct \emph{capabilities} was used. Specifically, the tool would need the following properties:

\begin{itemize}
  \item The ability to read .pdb files.
  \item The ability to read .cif files.
\end{itemize}

Probably the major Python PDB parser at time of writing is BioPython. This is a multi-purpose bioinformatics library, with many modules for dealing with general biological data formats. One of these modules allows for parsing of PDB structure files, and is frequently used in Python analyses of protein structures. However, BioPython's PDB reader does not support the processing of Biological Assemblies - it can only give the user the asymmetric unit as a model. As this project relies on having that functionality as a core part of the parsing process, this makes using BioPython difficult.

It was therefore decided to create a novel Python library for handling biological structures, which would focus solely on structural models (BioPython has many other disparate functions). An additional advantage to doing this was that it gave me a much greater insight into the structure of these files, and of structural biology concepts generally.

\section{Data Structures}

The object returned by the various parsing functions is a \texttt{File} object. This essentially represents the actual file object itself rather than its structural contents, and has attributes for PDB code, resolution, keywords, etc.

A \texttt{File} object has one or more \texttt{Model} objects.

\section{Parsing}

atomium can read .cif, .mmtf, or .pdb files. In each case the overall process is the same.

\begin{enumerate}
   \item Get the file contents as a string.
   \item Determine which filetype it is by looking at file extension or, if not possible, by looking at file contents.
   \item Convert the filestring to a Python dictionary whose structure is specific to that file type.
   \item Convert that dictionary to a standard atomium data dictionary, whose structure is the same regardless of the file type origin.
   \item Convert that data dictionary to an atomium \texttt{File} object with one or more \texttt{File}s within it.
\end{enumerate}

This process for parsing has a number of advantages over just trying to go from filestring to parsed object in one step. By making the three filetypes converge at one data structure - the atomium data dictionary - it prevents duplication of effort involved in going from `data' to `Python structure'. It also means that every file can have a consistent, dictionary representations, which means that they can all be represented as JSON if desired. It is also easier for testing, as each stage in this (relatively complex) parsing process can more easily be tested in isolation.

\subsection{File Contents and File Type Detection}

Getting the file contents as a string is a straightforward process - it is simply opened and read using the built in Python functions for that. The process is abstracted into a single function which will first try to read the file as a unicode string (appropriate for .cif and .pdb) and then as binary data (appropriate for .mmtf).

atomium also enables `fetching' a file from a remote server. Generally this is via HTTP, from the RCSB web servers \note{Cite this!} with a four letter code, though the user can select any URL. The Python library requests \note{Cite somehow} is used for this. Alternatively the user can fetch a file over SSH using a different function.

Once the file contents are extracted as a string (or bytestring), atomium then determines which of the three file types it has been given, so it knows which functions to use to process it further. This is done using a straightforward algorithm - if the file extension is given, this will be used to determine filetype. If not, or if the filetype is not one of the three it recognises (such as .ent for example), atomium will assume it is .mmtf if the data is binary, .cif if it is a unicode string that contains the string \texttt{\_atom\_sites}, and .pdb otherwise.

\subsection{File Dictionaries}

The next step is to convert this string into a Python dictionary that reflects the internal structure of that file type.

.cif files are essentially a list of connected tables - tables in the sense that each block has a list of headers, and then one or more lists of values that belong to those headers, and connected in the sense that a row may have an ID as its first value which is refered to in another table. atomium represents these tables as lists of dictionaires, where the headers are keys, and the row cells the values. Each of these table lists is a value in the top-level dictionary. \note{This definitely needs a figure or something to show what I'm talking about.} The main bottleneck in this process is splitting values on a single line - they are white-space separated, albeit with whitepsace within quotes ignored, and with both quote types permissable and escapable within the other. Initially the Python library shlex was used to do this splitting, but its speed was just too slow so a custom function was written which was faster - though still the slowest part of the process.

.mmtf are binary encoded MessagePack \note{cite} dictionaries, so once the Python msgpack library has decoded it, it is essentially already in dictionary form. The only extra steps that need to be taken are to decode the .mmtf formatted binary fields within this dictionary, using the algorithms specified in the .mmtf documentation and implemented in atomium.

.pdb files have much less internal structure than the other two, and are essentially just lists of records. These records can be grouped by record name however, and this is what atomium does - the file dictionary has record names as keys, and lists of records that belong to that record type as values. The only exceptions are \texttt{REMARK} records, which are further grouped by remark number, and all model related records (\texttt{ATOM}, \texttt{TER}, \texttt{HETATM} etc.), which are grouped together because their order relative to each other is information that would be lost of they were split up into record types (\texttt{TER} records separating chains being the main reason).

This resultant file dictionary, whose internal structure is specific to file type, is essentially just a Python representation of the file contents - no `parsing' is done as such, it just gets the information in a form that later processes can understand.

\subsection{Data Dictionary}

This file dictionary is then turned into an atomium `data dictionary'. This is a dictionary with a consistent internal structure regardless of what the original file type was, and essentially consists of a number of sub-dictionaries.

For example there is the description sub-dictionary, which contains meta information about the file such as title and deposition date. There is an experiment sub-dictionary, with information about how the model was created. The quality sub-dictionary contains resolution and other related information. The geometry sub-dictionary contains transformation matrices, such as those needed to create biological assemblies.

The most important one is `models', which is actually a list of sub-dictionaries. Each one contains informations about the chains, ligands and water molecules in that model, containing all the information needed to create an atomium \texttt{Model}.

Each of the three dile dictionaries have their own functions for extracting information from them and building a data dictionary. In the case of .cif and .mmtf file dictionaries, this is relatively straightforward, as much of the data has already been extracted in the previous step, and this is largely just a question of transfering data from one dictionary to another, and nesting atoms within the right structures. \note{Talk about chain ID problem}

For .pdb files however, the previous step just grouped records, and the data is still locked away within these strings at pre-defined offsets, so for this filetype it is this stage which is the bottleneck, as it takes longer to extract the necessary information.

\subsection{Model Generation}

The structure of the data dictionary is optimised to make model creation as rapid as possible. The polymers, ligands and water molecules are already segregated as they will be in the model, and all names and IDs are already parsed. Relevant \texttt{Model}, \texttt{Chain}, \texttt{Ligand} etc. molecules are created from this information.

The surrounding \texttt{File} object is made from the rest of the data dictionary, with the meta information, with the values of the sub-dictionaries becoming properties of that object. The \texttt{Model} objects exist as a list within that \texttt{File}, with a pointer to the first \texttt{Model} if the user only needs the first.

Biological assemblies can be created using a \texttt{generate\_assembly} fuction. In earlier versions of atomium, each individual molecule was transformed separately, but in very large structures this can be time consuming. Instead, the copies are created, and then the atoms of all the structures are transformed in one step.

\section{Saving}

One of the more unique features of atomium is that it allows changes to models to be made and then for those changes to be saved in \emph{any} of the three file types that atomium supports.

Unlike the process of parsing, in which the structures pass through a number of different representations reflecting the different layers of abstraction of that process, saving is much more starightforward. A function takes in an atomium \texttt{AtomStructure} (it doesn't need to be a model, chains etc. can be saved individually) and turns it into the relevant filestring.

Only structural information is saved - the various file annotations that are parsed from the original files are presumed to be properties solely of those original files, and not of any modified versions of them that might exist. So, atom records are saved, along with anything else needed to construct details of the actual structure model - but not titles, deposition dates, resolution etc.

\section{Usage}

%%%%%%%%%%%% END %%%%%%%%%%%%
