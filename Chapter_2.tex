%%%% MACRO DEFINITION %%%%

\providecommand{\pvivax}{P.~vivax}
\providecommand{\pfalciparum}{P.~falciparum}
\providecommand{\cterm}{C-terminus}
\providecommand{\nterm}{N-terminus}

\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\providecommand{\refimage}[1]{\figurename~\ref{fig:#1}}

%TC:macro \note [ignore]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%													BEGIN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Computational Techniques} % Write in your own chapter title
\label{Chapter2}
\lhead{Chapter 2. \emph{Computational Techniques}} % Write in your own chapter title to set the page header
In this chapter, the pre-existing algorithms and other computational techniques used to predict Zinc Binding Sites will be outlined.

It will not provide details of specific implementations of them used in this project, or any novel tecnhniques developed - those will be presented in later chapters. It is instead meant to give a general mathematical background to these techniques, and familiarise the reader with the terms that will be used in later chapters.

\section{Machine Learning}

\subsection{Principles}

This project aims ultimately to create a predictor - something which can take either a protein structure or a protein sequence, and determine where, if anywhere, are zinc binding regions.

Machine learning is the use of existing data to create predictive models that can take new data and make inferences about it. It `learns' the characteristics of the existing dataset and uses this to predict certain things about the new data. This can be `unsupervised', where the model simply breaks the data into clusters based on its internal structure, or it can be `supervised', where the existing data is labeled with some correct output, and the model learns how to predict the output based on the input characteristics. If the output is a discrete category, this is classification; if it is a continuous value, this is regression.

This project will use supervised learning, because the dataset being used to train the models will already have the locations of zinc binding sites labeled. The inputs and outputs are more open to interpretation however. So far I have framed the problem in terms of entire proteins - the input is an entire protein structure or an entire protein sequence, and the output is the number and location of the zinc binding site(s) that may be present. However this is difficult to translate into machine learning concepts, which is best suited to mapping input objects to either discrete classes or single continuous values. It is not really practical to represent `the number and location of the zinc binding site(s) that may be present' as a category or a number, so instead the problem will be re-framed so that the input is some sub-component of the protein, and the output is a binary class - `yes that component is a zinc binding region` or `no that region is not a zinc binding region'. The problem is therefore a classification problem, with two classes. When a protein is given, it is broken into its various components, each component is run through the model which predicts it as either zinc binding or not zinc binding, and then the net result for the protein as a whole is the list of components that are predicted as zinc binding.

But what should be the components? What inputs will actually be passed to the model? For protein structures, one obvious possibility is simply location in space - every point within the three-dimensional structure is taken (in some specified resolution grid) and passed to the model as an input, which assigns that location a predictive label. This would be comprehensive but time consuming, as there would be many points to check - the number of points could be limited to the protein surface, but there would still be a lot. Another possible means of breaking the structure into components would be to take all the combinations of residues that are known to bind zinc, and treat these as inputs - there could again be very many of these, but fewer than if every point in space were checked. For sequences, there is no equivalent to checking every point in space, as this information is obviously not available and residues nearby in space are not necessarily nearby in sequence. However using the combinations of zinc binding residues is still applicable to sequences.

When the inputs are passed to the model, they are done so as a set of `features'. These are numerical characteristics that the model actually uses to learn what kinds of input belong in which category. These are usually passed to the model as a vector of numbers - classification is just the mapping of vectors to discrete classes. How the inputs are actually represented is determined by feature selection, which must be carried out first. For example, while I referred to using locations in space as inputs in the preceding paragraph, the actual input vector itself wouldn't literally be the three Cartesian coordinates - if that were the case the model would simply try to associate certain distances from the origin of the PDB coordinate system with zinc binding, which would obviously be nonsensical. Instead you pick certain characteristics to measure for a point in space, such as (for example) `number of oxygen atoms within 3 ~{\AA}', and use that number in your input vector. The actual features used in this project will be outlined later.

However, too many features can be a problem. The more dimensions in the vector space the inputs inhabit, the more computationally expensive it can be to process them. Some supervised learning techniques, such as K-Nearest Neighbors, also have poorer performance at higher dimensions because data points are more `spread out' in higher dimensional space. For this reason, dimensionally reduction is often performed, whereby the dataset is transformed into one with fewer dimensions, but retaining the same predictive information. This can be either feature selection, whereby superfluous features which are already highly correlated with other features are removed, or feature extraction, where entirely new features that capture the essence of the original dataset are created.

Once the dataset has been processed, models must be trained. The simplest approach would be to give the model the entire dataset and let it learn from that. However, this can lead to a problem called overfitting, where the model learns details of the training set that are irrelevant - it learns the training set, but is not generalisable to objects not in the training set. To overcome this, the dataset is generally split into a training set and test set, with the latter being held in reserve to assess the model after it has been trained on the training set. If the model is overfit, it will perform much worse on the test set than the training set - if not, the performance on each will be broadly equivalent.

\subsection{Evaluation}

The test set is held in reserve so that the effectiveness of the model can be evaluated, but here `effectiveness' can be defined in a variety of ways. For this purpose, the model can produce one of two outcomes, true or false, so all of the test results fall into one of four categories - true positives (correctly identified as a binding site), false negatives (was a binding site, but not detected), false positives (not actually a binding site, but wrongly identified as one), and true negatives (not a binding site, not identified as one). The first and last categories are the `good' ones, the other two the errors.

The simplest means of grading a model is with accuracy - the total number of correct predictions (true positives and true negatives) as a proportion of all test inputs. This can be a misleading metric however, if the dataset is class-imbalanced - that is, if one class is much more prevalent than the others. This will be the case in this project, as non-zinc binding regions vastly outnumber zinc binding regions. Accuracy is misleading here, because a model which just always predicts the more-prevalent class can get a very high accuracy, despite being completely useless as a predictor of rare states.

Similarly the error rate - the total number of incorrect predictions (false negatives and false positives) as a proportion of all assignments - suffers from the same problem, and should be treated with the same caution in a class-imbalanced problem like this.

Two more meaningful metrics are precision and recall. Precision is the number of true positives, as a proportion of all predicted positives - that is, how many positive predictions were correct? Recall (also referred to as sensitivity) is the inverse, the number of true positives, as a proportion of all actual positives - that is, how many actual positives were identified? A high precision means that false positives are being kept down, with the reported positive cases being mostly correct. A high recall means that false negatives are being kept down, with most of the positive cases being identified.

However these two metrics exist in a balance, with improvements in one causing a decline in the other. The more `eager' the model is to assign a positive prediction to an input, the more of them will be identified and the higher recall will be - but a smaller proportion of the predicted positives will be correct, lowering precision. Likewise, the more resistant the model is to predictive a positive label, the higher precision will be, but the lower recall will be.

Usually it is convenient to have a single value representing the effectiveness of the model, which is done using the F1 Score. This is simply the`harmonic average' of precision and recall - the ratio of their product and sum, multiplied by two.

Another single value measure of model effectiveness is the Matthew's Correlation Coefficient, or MCC. This takes into account all four values in the confusion matrix, and has certain advantages over the F1 score in that it "which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories" \note{cite 10.1186/s12864-019-6413-7}. \note{Give formula}

There are metrics which show not how effective the model is in its current state, but how its ability to distinguish one class from another varies over different thresholds. Most classification algorithms can assign a probability of being in one class or the other, and the actual prediction is made by setting some threshold - if the probability is above this threshold, the positive case is predicted. If not, the negative case is predicted. A ROC Curve will show the model's effectiveness changes as this threshold is changed, by plotting false positive rate against true positive rate for every threshold value (to some given resolution). Purely random classifiers will produce a diagonal line - as the threshold is increased, both true positive predictions and false positive predictions will increase by equal amounts because the model can't actually distinguish one class from another at all. The better the model is at making this distinction however, the more this line will curve into the top-left corner, because true positives will increase much faster than false positives in a `good' model.

This graphical ROC curve can be represented in a single metric called the AUC - area under the curve. As the name suggests, this is the area underneath the ROC curve, which will be 0.5 for a purely random model, and 1 for a perfect model. The reason a perfect model has an AUC of 1, is because a perfect model will have some threshold which produces a true positive rate of 1 and a false positive rate of 0, meaning it will touch the top left corner of the plot.

Another means of evaluating a model is examine how its performance increases with increases in training set size --- learning curves. In this analysis, the model is trained in random subsets of the training data of increasing size, and the training and test scores taken at each stage --- the score used depends on what is being assessed. This is useful in assessing the rate of convergence (how the model's training performance is approaching the test performance).

\subsection{Supervised Learning Algorithms}

Supervised learning is the mapping of input vectors to discrete output categories, but there are multiple methods and algorithms for doing this. Each of them is suited for different kinds of problem, and they will be outlined here.

The most basic machine learning algorithm tries to find the parameters to some function of the input values, and then finds a threshold above which the positive class should be predicted. For example, some of the earliest methods to be developed were the Perceptron and Adaptive Linear Unit. These create a linear function, with parameters that are learned during training. In this case `learning' means that the output of this function is evaluated for every input in the training set, the sum of the squared errors is found (which acts as a score of how poorly the model has performed with its current parameters), and the derivative of this squared error function is used to change the values of the parameters using gradient descent. Gradient descent is a common optimisation algorithm that tries to find the minimum of a function - sum of squared errors as a function of input vector, in this case. In practice, a variant of gradient descent called stochastic gradient descent is generally preferred, where a random subset of the training data is used each time. This speeds up the rate of optimisation \note{Why?}.

Linear perceptrons are relatively primitive - they use a linear function and so can only separate data that is linearly separable in the first place, meaning that a line, or plane (etc.) can divide the data in space. However they do illustrate many of the principles that are common to most or all supervised learning algorithms. Many of them have the overall architecture of a function with learned parameters and a threshold, for example. They also illustrate the difference between parameters and `hyperparameters'. A parameter is something the algorithm itself learns, and in this case the parameters are the coefficients of the linear equation, and the threshold value. So for n-dimensional input vectors, there will be n + 1 parameters to learn. A hyperparameter on the other hand is a setting that the user manually sets. For example, the number of passes through the data to make (epochs).

A more sophisticated variant of the perceptron is a supervised learning algorithm called logistic regression. In logistic regression, you still have a linear function whose coefficients the system tries to learn using (usually stochastic) gradient descent. However in this algorithm, before checking the output of the function against a threshold, the value is passed into a second `activation function' which is not linear. In this case the activation function is a sigmoid-like function that maps all inputs to some value between -1 and 1.

Not all classification algorithms take this form of fitting the parameters of a function and then applying a threshold cutoff. Decision trees, for example, divide the input space into regions by a series of branching cutoffs. At the first `node', a dimension and a value which maximally separates the categories is picked, bifurcating into two branches. For each of these two branches, another bifurcation takes place, and so on until the categories are separated. The number of nodes - the depth - is a hyperparameter that is chosen beforehand. This can be a very important consideration - too few nodes, and the algorithm will usually not be able to cleanly separate the data, with low recall and/or precision being achieved. However if there are too many branches, the model will become overfit - it may perfectly separate the training data, but in doing so will learn statistical noise present only in that sample, reducing its ability to generalise.

An extension of the decision tree that can be very effective is the random forest classifier. This algorithm generates multiple decision trees (the precise number being a tunable hyperparameter), each of which is trained on a random subset of the available training data. When a new input is to be classified, each of the trees classifies it according to its own training, and a majority vote is taken to determine the forest's verdict as a whole. This is an example of `bagging', where the noise and errors of individual models are canceled out in an averaging. \note{Is that actually the definition?}

Another supervised learning classification algorithm is K-Nearest Neighbours. This is a slightly unusual algorithm as it has no parameters that are tuned during training. Instead `training' in this case simply means giving it the entire training dataset, which it stores, and this entire dataset is used when classifying new inputs. The k closest vectors in the training set to the input to be classified (the k nearest neighbours) and identified using some distance metric (typically standard Euclidian distance) and a majority vote is taken - the category most represented in the nearest neighbours is the predicted category for the input. The main hyperparameter for this algorithm is the value of k - the number of neighbours to look for. An odd number is usually preferable, so that for binary classification problems there will always be a winner of the majority vote.

K-Nearest Neighbours illustrates a problem that is present in many machine learning algorithms - the so-called `curse of dimensionality'. That is - the more dimensions a dataset has, the harder it is to create effective, predictive models. This is not just because of the extra computational burden. Higher dimensional data is more `sparse' - the distances between objects gets larger. In the case of K-Nearest Neighbours this can be a problem because the nearest neighbours are further away, and the distance of the nearest neighbours approaches the average distance between data points as dimensionality increases, reducing the information content in those neighbours.

There are techniques which can be used to reduce the number of dimensions in a dataset, and they fall into two categories - feature selection and feature extraction.

Feature selection methods identify features in the dataset which can be removed with minimal loss of information. Some features may be very highly correlated with each other for example, meaning that one of them is redundant. Sequential Backward Selection (SBS) is an example of this - you specify the number of dimensions that you want, and remove dimensions one by one until you get to this number. At each iteration, the dimension to remove is determined by some pre-determined criterion, such as which feature has the smallest effect on the model's predictive ability when removed.

\note{Mention Random Forest's ability to do feature selection?}

Feature extraction, by contrast, does not use a subset of the original features, but rather creates an entirely new set of features, transformed from the original ones, that contains the same information in a lower dimensional space. Principal Component Analysis (PCA) is a widely used example, which transforms the original data such that the direction of maximum variance in the original dataset becomes a dimension in the new dataset. This requires the data to be normalised - scaled so that all features are on the same order of magnitude, and with a normal distribution centered at zero. \note{Explain this much better, and the other techniques}

The above supervised learning methods are established algorithms for classifying inputs into categories using single functions. They are widely used, and have been used with considerable success in bioinformatics problems - including the identification of zinc and other metal binding sites (see below). However, there are limits to the discriminatory power of a single function, linear or otherwise, and hence limits to the complexity of the underlying system they can model. Their `capacity' - the space of possible functions they can assume to model reality - is limited.

Rather than utilising a single function, deep learning uses nested functions - each with its own set of parameters - to capture different layers of abstraction in the input data. There is an input layer, which maps the input vector to some intermediate vector which represents some aspect of the input data. One or more hidden layers then acts on these intermediate vectors, eventually passing the data to the output layer which produces a vector representing the model's degree of certainty that the input belongs to each of the possible categories. Each layer acts as a kind of perceptron with a non-linear activation function - if they were all linear then the model as a whole would be linear. This architecture is a neural network, and this is the classical implementation, with every layer fully connected to the layer before it.

Training a neural network requires finding the optimal values for the weights and threshold for each layer. Using the usual technique of defining a cost function and the differentiating this over the various layers would be very computationally difficult to do with the necessary precision. Fortunately a shortcut exists called backpropagation, which allows the neural network to be trained with much fewer computational resources.

There are variants of the artificial neural network (ANN) model outlined above which are better served for different kinds of tasks. Convolutional Neural Networks (CNNs) are a widely used variant that are optimised for processing images and other input spaces that can be thought of as regularly spaced grids in n dimensions. Rather than having every layer fully connected to the layer before it, there are a number of `convolutional' layers in which the perceptrons only connect corresponding localised regions - followed by normal fully connected layers at the end. Rather than standard matrix multiplication, a mathematical operation called convolution is used to determine the forward flow of information through the network.

Another variant is the Recurrent Neural Network (RNN). In standard neural networks, the input data is unordered, and each input is treated in isolation - the network `forgets' what it has just done and treats each input like it was the first. However often datasets have inherent order, and knowledge of what the last input was classified as (or what the last n classifications were) contains important, usable information for classifying the current input. Recurrent neural networks are usually used for sequential data therefore, and are therefore of use in dealing with biological sequences (see below). They operate by adding an extra term relating to previously classified inputs. \note{Explain LSTMs}

Deep Learning is a kind of representational learning - the model itself is learning the features in the input space that are important. This can be seen when processing images - the model's first layer may look for low-level properties of the image, such as regions of high contrast representing borders, the next layer may look at simple combinations of these, and later layers will be sensitive to more high level combinations of visual features. This is inspired by - though does not work in precisely the same way as - the mammalian visual cortex.


\section{Machine Learning to Predict Zinc Binding}

There are still many proteins whose function is unknown. In many cases we only know their sequence, though some of these have had their structure solved experimentally.

One means of determining what function a protein might be intended to perform is to look for certain motifs associated with a given function. If, for example, it can be shown that such a protein had one or more zinc binding sites, that could offer insights into what function the protein performs. Indeed, that is ultimately the goal of this project.

Owing to this clear usefulness, there have been a number of attempts to develop such predictors in the past, dating back to the early 1990s. These methods and techniques form the essential context in which my own project should be understood, and also provide the benchmark of success against which my methods should be judged. Here their progress will be reviewed. They can essentially be divided into two broad categories - prediction from structure and prediction from sequence.

\subsection{Prediction from Structure}

Predicting zinc binding from structure takes the atom coordinates of some protein as its input, and produces as output residues in that structure which are believed to be able to bind zinc if zinc were present - generally along with some estimated probability that this should be the case.

This is perhaps of only limited utility, as very often proteins which can bind zinc tightly will have zinc bound when the structure is solved, and so no prediction is required. Where such methods are of use however is when a protein capable of binding zinc is crystallised without zinc being present - presumably because the researchers didn't know it could bind zinc and so did not provide any in the medium. These apoproteins can be scanned by predictors like this to identify unfilled zinc binding sites. \note{NMR invisibility?}

\note{Other uses?}

The earliest attempt at a general purpose prediction algorithm was the Hydrophobic Contrast Function, developed in 1990 \cite{yamashita1990metal} to detect metal binding sites in general. This began from the observation that metal atoms tended to be surrounded by a shell of hydrophilic atoms, which in turn were surrounded by an outer shell of hydrophobic atoms. They developed a function which took as its input a coordinate in space, and returned a measure of this `hydrophobic contrast' at that location. They showed that by scanning every point in the structure on a grid, evaluating the function at every point, and then ranking the points by their score (the function returned values between -1 for inverted contrast and 1 for desired contrast), the highest scoring points would be clustered at sites of metal binding.

This was the first demonstration that a property of metal binding sites could be used to create a predictive model. However it should be noted that it was only tested on structures with metals already present, not on apoproteins, and the initial observation itself was based on a very small number of metal proteins that were available at that time. It was later expanded upon by combining it with a template-based searching algorithm, whereby the known metal binding sites were represented as stripped-down `templates' of just alpha and beta carbons, and used to search proteins using a simple pattern matching algorithm \cite{gregory1993prediction}. Once a set of three residues matching a known template was found, the hydrophobic contrast function was applied to verify that the binding site was a region of high contrast, which it generally was. This was part of a pipeline for engineering binding sites into proteins, so it didn't require the matching residues to be of any particular type, but the general principle of using known site geometry and hydrophobic contrast properties to look in new structures was still in use.

MetSite was one of the earliest use of machine learning techniques to predict binding sites as we would understand the term today, in 2004 \cite{sodhi2004predicting}. They used artificial neural networks to detect metal binding sites in low-resolution structural models, such as those generated computationally from sequence data. They set themselves a target of having a false positive rate no higher than 5\%, and using this threshold were able to get an 
accuracy of 94.2\% and, more informatively, a recall of 60\% - across all sites. These figures fall to 84\% and 39\% for specific metals, from which they infer that many metals in the structures used are labelled incorrectly. They show that their model can predict metal binding in proteins of unknown function, and in models generated computationally, because the features they use don't use information from the precise positions of liganding atoms, but from outer residues and secondary structure.

Not all attempts at structural prediction have used straightforward supervised learning. The field of molecular dynamics has made important contributions, through the use of empirical force fields which can describe the attraction that a small molecule should experience towards a region of a protein. One such force field, the Fold-X empirical force field, was adapted for use on single atom ligands such as water and metal atoms \note{cite}. Not only were they able to 'decorate' protein surfaces with identified metal binding sites, but the implied affinity of the binding sites reported by the force field correlated closely with experimentally reported affinities where available.

Simple geometric templates have also been employed to identify metal binding sites. Calcium binding sites have been successfully predicted by simply looking for clusters of oxygen atoms and identifying their centre \cite{deng2006} for example. In 2008, Goyal and Mande built up more complex templates using pairs of residues to identify previously unknown 
sites.

Generally however, the field is dominated by the use of widely used machine learning algorithms. An important development in 2008 showed that decision trees and support vector machines could be used to identify metal binding sites in apo-protein structures - that is, structures solved in the absence of the metal in question \cite{babor2008}. This was an important proof-of-concept as it demonstrated that in the absence of the metal atom, the liganding residues are still sufficnetly close to their liganding conformation to allow them to be identified from atom positions. A similar study in the same year showed that Bayesian classifiers alone could identify zinc binding sites specifically, using only concentric shells of atoms around the site as features \cite{ebert2008}.

\emph{CHED}

\emph{FEATRUE}

\emph{SitePredict}


One of the more recent algorithms for identifying potential zinc binding sites in protein structures is TEMSP \cite{zhao2011structure}. This is actually reminiscent of some of the earlier work on this problem, in that it uses simple geometric patterns of known zinc binding sites - specifically their alpha and beta carbons - to search target structures without any particular machine learning method employed (though there is a certain amount of parameter training). Here the problem is simplified by creating a library of `residue pairs', where for each zinc binding site obtained from the PDB, they store each pairwise combination of liganding residues (actually just certain properties of them, such as alpha-alpha distance etc.) and then search the target protein for these. Once one is found, other template pairs are superimposed on the structure to find additional liganding residues. They used a stricter definition of `true positive' than previous attempts, and still reported a sensitivity of 86.0\% and a selectivity of 95.9\% in their test dataset - barely lower than the values obtained when testing on the training dataset used to refine their parameters. When applying their tool to a dataset of proteins of unknown function, they found some very promising results - finding 186 probable zinc binding sites. \note{So what?} Unfortunately, the online version of this tool is no longer accessible at time of writing.

\subsection{Prediction from Sequence}

A system for identifying zinc binding residues from the mere sequence of residues it contains is arguably much more useful, as protein sequences are much more readily available. It allows for whole genomes to be scanned for zinc binding proteins. As such, there has been much effort in the past to create techniques for predicting zinc binding from sequence.

The earliest research in this area was largely a `manual' effort, given the small number of sequences available. The discovery of zinc finger proteins in 1985, for example, involved the observation of repeating C...C...H...H patterns in the proteins being studied, and the hypothesis that these were binding zinc \cite{miller1985repetitive}. Similarly, zinc binding in the E6 and E7 subunits of a Papillomavirus was predicted from the observation of cysteine repeats \cite{barbosa1989papillomavirus}. Slightly more complex patterns were later identified, such as the observation that enzymatic zinc binding sites frequently have two residues close to each other in the sequence, and then a third much more distant --- the `short and long spacers` \cite{vallee1989short}. This kind of manual examination of individual protein sequences is obviously not scalable to the vast numbers of protein sequences now available, and can only identify relatively obvious binding sites in any case --- but the same principles that were used in these early studies of characteristic repeats largely underpins the automated techniques now in use.

As the number of sequences grew, identifying zinc binding sites through the use of sequence alignment to known zinc binding sequences became possible. This was used to identify plausible sites in Protein Kinase C \cite{bishop1991identification} and in {\it Giardia lamblia} surface proteins.

One of the first uses of traditional machine learning methods was in 1995, when neural networks were employed to predict zinc finger proteins from sequence data \cite{nakata1995prediction}. They hardcoded the specific C...C...H...H spacer pattern they were looking for into the model, rather than having the model learn it from existing sequence data by encoding it as a binary feature, and also incorporated features for electric charge, hydrophobicity, and side chain types. Overall this was largely successful - they reported an accuracy of 97\%, though for a very specific type of zinc binding sequence that was largely hardcoded in from the start.

One of the ultimate advantages of sequence prediction over structure prediction is that it allows you to quickly and efficiently search very large datasets for evidence of zinc binding. Again, early efforts relied on sequence alignment --- in 2004 every then-known metal binding sequence was encoded as a series of residue codes and gap sizes (referred to as `metal binding patterns'), and these were then BLAST searched against entire genomes of various species \cite{andreini2004hint}. The same team would go on to study zinc binding in the human genome specifically using a refined version of this technique and assert, based on this method, that the human genome encodes 2,800 +/- 400 zinc binding proteins --- about 10\% \cite{andreini2006counting}. Shortly afterwards they expanded their analysis to representative genomes from eukaryotes, prokaryotes and archaea, and found that this proportion varies from 4\% to 10\% --- with eukaryotes having more due to higher numbers of regulatory proteins \cite{passerini2007predicting}.

Studies using traditional machine learning techniques have typically used a mix of simpler methods such as Support Vector Machines and `deep' methods, such as neural networks and their variants. Both SVM and neural networks were used to predict the metal binding cysteine and histidine residues of sequences by the same group that developed the metal binding pattern concept --- though it relies on the sites present in the searched sequence belonging to previously identified motifs, as it is these previously identified patterns that their models were trained with \cite{passerini2006identifying}.

\section{Web Technologies}

A major part of this project's utility to the wider scientific community is the presentation of the data and tools created for it to the world. All of them are available via the web, and various web technologies are used to accomplish this.

\note{GitHub/Git? Django->Python/Django? React-?Javascript/react?}

\subsection{Django}

Django is an open source Python web framework, used to create database-backed web applications \cite{django}. All database population scripts are written in Python, and use the Django Object-Relational Mapping utilities to populate the database.

\subsection{GraphQL}

GraphQL is an API technology and query language which allows users to construct requests for precisely the data they need - no more, no less \cite{graphql}.

Traditionally, web APIs have used the REST archictecture, whereby every meaningful object is given a URL address. The problem that GraphQL seeks to address with REST is that REST APIs often involve overfetching (you get the full representation of an object whether you want it or not) and underfetching (you have to make multiple requests to get the data you need). With GraphQL APIs, you define your objects using a graph of relationships, and then request exactly the data you need.

GraphQL APIs are used here to provide computational access to both the dataset itself, and to the predictive models.

The Python/django library graphene is used here to implement the GraphQL schemas and API services \cite{graphene}.

\subsection{React}

While the web services provide computational access are useful for programmatic access, for browsing the data initially a website frontend is much more useful. In order to avoid duplication of logic, a JavaScript framework which connects to the existing API is used here - React \cite{react}.

React uses individual components to build up a single page web application, and the React router package is used to create individually addressed pages.

Other JavaScript technologies used on the frontend are the 3D macromolecular visualisation package NGL (used for visualising the zinc binding sites) \cite{rose2015ngl} and the charting library highcharts (used to give an overview of the data) \cite{highcharts}.

\section{Conclusion}

Predicting zinc binding sites is a binary classification task --- all potential sites in a protein, whether that is defined as a location in space or a combination of residues, must be assigned to the category of `true zinc binding site' or `non zinc binding site', using information about what distinguishes these categories from a dataset of confirmed zinc binding sites.

This is a task for which there are a variety of well established machine learning techniques applicable, the basic theory, advantages, and scope for customisation of which have been outlined here. There is a history of such techniques being applied to the problem, the results have which have also been reviewed here --- though few if any are open source and/or still publicly available over the web as they were at the (often quite recent) time of their initial publication.

These are the computational techniques which will be used to build the predictive models. To make these models available to researchers, a variety of modern web technologies will also be used. Before any predictive models can be utilised however, the dataset which will ultimately form the training data must be built.