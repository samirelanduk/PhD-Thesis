%%%% MACRO DEFINITION %%%%

\providecommand{\pvivax}{P.~vivax}
\providecommand{\pfalciparum}{P.~falciparum}
\providecommand{\cterm}{C-terminus}
\providecommand{\nterm}{N-terminus}

\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\providecommand{\refimage}[1]{\figurename~\ref{fig:#1}}

%TC:macro \note [ignore]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%													BEGIN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Computational Techniques} % Write in your own chapter title
\label{Chapter2}
\lhead{Chapter 2. \emph{Computational Techniques}} % Write in your own chapter title to set the page header
In this chapter, the pre-existing algorithms and other computational techniques used to predict Zinc Binding Sites will be outlined.

It will not provide details of specific implementations of them used in this project, or any novel tecnhniques developed - those will be presented in later chapters. It is instead meant to give a general mathematical background to these techniques, and familiarise the reader with the terms that will be used in later chapters.

\section{Machine Learning}

This project aims ultimately to create a predictor - something which can take either a protein structure or a protein sequence, and determine where, if anywhere, are zinc binding regions.

Machine learning is the use of existing data to create predictive models that can take new data and make inferences about it. It `learns' the characteristics of the existing dataset and uses this to predict certain things about the new data. This can be `unsupervised', where the model simply breaks the data into clusters based on its internal structure, or it can be `supervised', where the existing data is labeled with some correct output, and the model learns how to predict the output based on the input characteristics. If the output is a discrete category, this is classification; if it is a continuous value, this is regression.

This project will use supervised learning, because the dataset being used to train the models will already have the locations of zinc binding sites labeled. The inputs and outputs are more open to interpretation however. So far I have framed the problem in terms of entire proteins - the input is an entire protein structure or an entire protein sequence, and the output is the number and location of the zinc binding site(s) that may be present. However this is difficult to translate into machine learning concepts, which is best suited to mapping input objects to either discrete classes or single continuous values. It is not really practical to represent `the number and location of the zinc binding site(s) that may be present' as a category or a number, so instead the problem will be re-framed so that the input is some sub-component of the protein, and the output is a binary class - `yes that component is a zinc binding region` or `no that region is not a zinc binding region'. The problem is therefore a classification problem, with two classes. When a protein is given, it is broken into its various components, each component is run through the model which predicts it as either zinc binding or not zinc binding, and then the net result for the protein as a whole is the list of components that are predicted as zinc binding.

But what should be the components? What inputs will actually be passed to the model? For protein structures, one obvious possibility is simply location in space - every point within the three-dimensional structure is taken (in some specified resolution grid) and passed to the model as an input, which assigns that location a predictive label. This would be comprehensive but time consuming, as there would be many points to check - the number of points could be limited to the protein surface, but there would still be a lot. Another possible means of breaking the structure into components would be to take all the combinations of residues that are known to bind zinc, and treat these as inputs - there could again be very many of these, but fewer than if every point in space were checked. For sequences, there is no equivalent to checking every point in space, as this information is obviously not available and residues nearby in space are not necessarily nearby in sequence. However using the combinations of zinc binding residues is still applicable to sequences.

When the inputs are passed to the model, they are done so as a set of `features'. These are numerical characteristics that the model actually uses to learn what kinds of input belong in which category. These are usually passed to the model as a vector of numbers - classification is just the mapping of vectors to discrete classes. How the inputs are actually represented is determined by feature selection, which must be carried out first. For example, while I referred to using locations in space as inputs in the preceding paragraph, the actual input vector itself wouldn't literally be the three Cartesian coordinates - if that were the case the model would simply try to associate certain distances from the origin of the PDB coordinate system with zinc binding, which would obviously be nonsensical. Instead you pick certain characteristics to measure for a point in space, such as (for example) `number of oxygen atoms within 3 ~{\AA}', and use that number in your input vector. The actual features used in this project will be outlined later.

However, too many features can be a problem. The more dimensions in the vector space the inputs inhabit, the more computationally expensive it can be to process them. Some supervised learning techniques, such as K-Nearest Neighbors, also have poorer performance at higher dimensions because data points are more `spread out' in higher dimensional space. For this reason, dimensionally reduction is often performed, whereby the dataset is transformed into one with fewer dimensions, but retaining the same predictive information. This can be either feature selection, whereby superfluous features which are already highly correlated with other features are removed, or feature extraction, where entirely new features that capture the essence of the original dataset are created.

Once the dataset has been processed, models must be trained. The simplest approach would be to give the model the entire dataset and let it learn from that. However, this can lead to a problem called overfitting, where the model learns details of the training set that are irrelevant - it learns the training set, but is not generalisable to objects not in the training set. To overcome this, the dataset is generally split into a training set and test set, with the latter being held in reserve to assess the model after it has been trained on the training set. If the model is overfit, it will perform much worse on the test set than the training set - if not, the performance on each will be broadly equivalent.